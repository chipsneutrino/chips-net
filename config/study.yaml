task: "study"  # What is the task? (create, train, study)
exp:
    name: "study"  # Name of experiment
    output_dir: "./data/models/"  # Path to model output experiment directory
    comet: False  # Should we use comet to store the experiment online
data:
    input_dirs:  # The directories from which to take input data
        #- "./data/input/chips_1200_all/beam_flux/"
        - "./data/input/chips_1200_all/nuel_cccoh/"
        - "./data/input/chips_1200_all/nuel_nccoh/"
        - "./data/input/chips_1200_all/nuel_ccdis/"
        - "./data/input/chips_1200_all/nuel_ncdis/"
        - "./data/input/chips_1200_all/nuel_ccqel/"
        - "./data/input/chips_1200_all/nuel_ncqel/"
        - "./data/input/chips_1200_all/nuel_ccres/"
        - "./data/input/chips_1200_all/nuel_ncres/"
        - "./data/input/chips_1200_all/nuel_ccmec/"
        - "./data/input/chips_1200_all/numu_cccoh/"
        - "./data/input/chips_1200_all/numu_nccoh/"
        - "./data/input/chips_1200_all/numu_ccdis/"
        - "./data/input/chips_1200_all/numu_ncdis/"
        - "./data/input/chips_1200_all/numu_ccqel/"
        - "./data/input/chips_1200_all/numu_ncqel/"
        - "./data/input/chips_1200_all/numu_ccres/"
        - "./data/input/chips_1200_all/numu_ncres/"
        - "./data/input/chips_1200_all/numu_ccmec/"
        #- "./data/input/chips_1200_all/cosmic/"
    #channels: [1, 1, 1, 0, 0, 0, 0]  # Which channels should be active?
    channels: [1, 1, 1]  # Which channels should be active?
    seperate_channels: True  # Should we split the channels into seperate images?
    img_size: [64, 64]  # The input image size
    augment: True  # Should we enable absolute and factor augmentation of the input images?
    aug_factor_mean: [0.0, 0.0, 0.0]  # Scale bin contents by a factor, mean
    aug_factor_sigma: [0.02, 0.02, 0.02]  # Scale bin contents by a factor, sigma
    aug_abs_mean: [0.0, 0.0, 0.0]  # Shift bin contents by an absolute value, mean
    aug_abs_sigma: [0.0, 0.0, 0.0]  # Shift bin contents by an absolute value, sigma
model: 
    type: "vgg"  # (vgg, resnet, inception)
    #labels: ["t_cosmic_cat", "t_vtxX", "t_vtxY", "t_vtxZ"]  # For cosmic classifier
    labels: ["t_nu_nc_cat"]  # For beam category classifier
    #labels: ["t_nu_energy", "t_lep_energy"]  # For energy estimation
    lr: 0.0002  # The initial learning rate
    lr_decay: 0.5  # Learning rate decay coefficient
    dense_units: 512  # Number of units to use in dense layers
    dropout: 0.1  # Dropout rate to use in dropout layers
    filters: 64  # Initial number of filters to use
    reco_pars: True  # Should the reco paramters be appended to final dense layer?
    se_ratio: 16  # Ratio for squeeze-exitation blocks, if zero they will not be used
    learn_weights: False  # If training multiple labels, should we use the custom weight learning layer?
    precision_policy: "mixed_float16"  # Should we use mixed precision 'mixed_float16' or normal 'float32'
    summarise: True  # Should we display a summary of the model when it is built?
trainer:
    train_examples: 1280000  # Number of training examples to use
    val_examples: 32000  # Number of validation examples to use
    test_examples: 128000  # Number of testing examples to use
    batch_size: 128  # Training batch size
    epochs: 5  # Number of epochs to train for
    tb_update: 1000  # How often should tensorboard record values (batches)
    steps_per_epoch: -1  # Number of steps per epoch (-1 default)
    es_monitor: "val_accuracy"  # Metric to monitor for early stopping
    es_delta: 0.001  # Early stopping minimum delta value
    es_epochs: 2  # Early stopping epochs without delta change before stop
study:
    trials: 50  # Number of trials in the study
    # The following list the SHERPA options for variables in the previous sections
    data:
        unstack: [True, False]
        augment: [True, False]
    model:
        lr: [0.00005, 0.001]
        lr_decay: [0.5, 0.5]
        dense_units: [255, 512]
        dropout: [0.0, 0.25]
        filters: [32, 64]
        reco_pars: [True, False]
        se_ratio: [0, 16]
        learn_weights: [True, False]
    trainer:
        batch_size: [64, 128, 256]